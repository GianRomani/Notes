1.  What is batch normalization? What are its drawbacks? Describe normalizations that rectify them.
2.  Explain bias-variance trade-off.
3.  List common generative models and their pros and cons.
4.  Why is it difficult to train vanilla GANs? How can you improve the training.
5.  How would you prevent a neural network from overfitting?
6.  Explain how to apply drop-out. Does it differ for train and test?
7.  Give examples of neural-network optimizers, how do they work? What is gradient explosion? How to rectify it with activation functions?
8.  Why neural networks might not be suitable for a given task? ([check out this article](https://towardsdatascience.com/seven-questions-to-ask-before-introducing-ai-to-your-project-b969d591c98b))
9.  Explain attention mechanism in transformers (these are now a general framework that can be applied to a wide range of problems).
10.  Describe a dimensionality reduction technique of your choice.

**Computer Vision questions**:
1.  Why CNNs are so good when applied to images?
2.  How to apply deconvolution?
3.  Why transformers are challenging to apply to images?
4.  How would you map emotions from one face to another?
5.  What is temporal consistency and how to achieve it?
6.  What loss functions can you use for a generative model?
7.  Why FID score might is not the best realism metric?
8.  Describe convolution types and the motivation behind them.
9.  What is instance normalization?
10.  Describe loss functions for image reconstruction and their pros and cons.

**NLP questions**:
1.  Why do we need positional encoding in transformers?
2.  Describe several attention mechanisms, what are advantages and disadvantages?
3.  What are the advantages of LSTMs versus vanilla RNNs?
4.  What would we expect from a good embedding?
5.  How to evaluate translation quality?Â [Why BLEU is not a very good metric?](https://medium.com/towards-data-science/even-google-does-not-get-it-right-a9685f7f63c9)
6.  What is contrastive predictive coding, and why is it helpful?
7.  What techniques for NLP data augmentation do you know?
8.  How to learn rare words when training models?
9.  Explain how language can be viewed as a graph. What properties would the graph have?

**System questions**:
1.  How would you tell which one of two models is better?
2.  How would you design a video in-painting/face transfer/normal map reconstruction (this is from computer graphics)/spam detector algorithm from scratch?
3.  How to deal with data-drift in the deployed model?
4.  How to design an algorithm, that given an image of damaged fence would estimate the insurance cost?


### Helpful repositories on GitHub
1. [Interview questions and answers](https://github.com/andrewekhalel/MLQuestions)
2. [MLE Flashcards](https://github.com/b7leung/MLE-Flashcards)
3. [Learning ML](https://github.com/rentruewang/learning-machine)
4. [Data Science CheatSheet](https://github.com/aaronwangy/Data-Science-Cheatsheet)

