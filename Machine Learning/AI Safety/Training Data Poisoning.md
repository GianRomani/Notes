Created: 2024-03-12 12:02
#quicknote

One of the [[Vulnerabilities in LLM-base applications]].

Training data poisoning involves the intentional manipulation of data used for either initial model training, fine-tuning, or embedding processes. Attackers aim to introduce vulnerabilities, backdoors, or biases into the model, potentially leading to:

- **Security Compromises:** Poisoning can enable unauthorized actions or the leakage of sensitive information.
- **Performance Issues:** Deliberate degradation of the model's accuracy or reliability.
- **Ethical Concerns:** Introduction of harmful biases that reflect in the model's output.
- **Downstream Risks:** Exploitation of vulnerabilities in software dependent on the poisoned model.
- **Reputational Damage:** Loss of trust due to security breaches or biased outputs.

## Resources
1. [OWASP](https://owasp.org/www-project-top-10-for-large-language-model-applications/assets/PDF/OWASP-Top-10-for-LLMs-2023-v1_1.pdf)

#### Tags
#aisecurity #llm #cybersecurity 