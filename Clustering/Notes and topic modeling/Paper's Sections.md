# Paper's Sections
Created: 2022-09-12 14:07
#note

## 2) Background
**Topic modeling** methods can be applied to a great variety of fields and data types, including information retrieval ([paper](https://maroo.cs.umass.edu/getpdf.php?id=850)), bioinformatics ([review](https://springerplus.springeropen.com/articles/10.1186/s40064-016-3252-8)) and topic discovery in images ([paper](https://ieeexplore.ieee.org/abstract/document/1541280.?casa_token=CuY86UvTQWwAAAAA:Mh1AaYah3cD5a6lZVorn2Qj_OXvg76RosG1rSdVW5INWzq3NWIVYwWdMRZcVZnrxuJDH)). Our work focuses on approaches that can be used for text-based data.

### 2.1) Definition of Terms

In this section, we provide definitions on terms and basic concepts involved in topic modeling.
A typical text-based dataset is made of “documents” which are strings of variable length composed of *N* words, where a “word” (or “term”) is considered as the fundamental unit of a sample. The set of distinct words presents in a dataset forms a “vocabulary” and a “topic” is then viewed as a probability distribution over this fixed vocabulary.
Obviously, the way in which we represent words and documents has a great impact on topic modeling. We will then present the ideas that are useful to understand the approaches we are analysing in this work. We will refer to the classification of word representation's techniques proposed by [S. Selva Birunda and R. Kanniga Devi](https://link.springer.com/chapter/10.1007/978-981-15-9651-3_23).

Category 1: Traditional word embedding, or Count-based embedding [here](https://arxiv.org/pdf/1901.09069.pdf). In this class fall all those methods that use frequency of words on the whole document, co-occurrence of words, and rarity of words in documents. Traditionally, text documents are represented as a bag of words, i.e. each document is described by a vector of dimension equal to the vocabulary size, where each dimension represents the number of times a certain word appears in a document. The limits of such text representation are known: the vectors are very sparse, if we add a new document with words never used before the length of the vocabulary, and so of the vectors, will increase, and the context is not considered. 
A first improvement is given by TF-IDF, which measures how frequent a word is in a document (TF) and how much information it provides (IDF). 
The well-known formula for TF-IDF is: $tfidf_{t,d}=tf(t,d) \cdot idf(t,D)$, with $tf(t,d)=\dfrac{f_{t,d}}{\sum_{t' \in d}f_{t',d}}$ and $idf(t,D)=\log \dfrac{|D|}{1+|\{d \in D : t \in d\}|}$ where $f_{t,d}$ is the raw count of term *t* in the document *d* and *D* is the dataset. The i-th document is then represented as $d_i=[tfidf_{0,i},...,tfidf_{N,i}]$, where N is the number of words in the vocabulary.

Category 2: Static Word Embedding. These prediction-based methods compute probabilities to the words and map them into fixed-size vectors. These embedding do not consider context, i.e. a word embedding does not change if the word appear in a different sentence. If two words often appear together, then their embeddings will be similar.
This class of techniques gained in popularity after the release of Word2Vec [here](https://www.cambridge.org/core/journals/natural-language-engineering/article/word2vec/B84AE4446BD47F48847B4904F0B36E0B). This model can utilize either of two architectures: continuous bag-of-words (CBOW), which predicts one word from the surrounding words, or Skip-gram, that, on the other end, uses one word to predict all surrounding words. Word2Vec's idea has been used to design Doc2Vec ([paper](https://arxiv.org/pdf/1405.4053.pdf)), an algorithm that can create a numeric representation of a document, regardless of its length.

Category 3: Contextualized Word Embedding. Since context is considered in these models, the word representation dynamically varies based upon the surrounding words.
Models that use this kind of representation, like Transformers, are SOTA for most NLP tasks.These approaches are context-dependent, i.e. they can disambiguate polysemes, thanks to the [attention](https://arxiv.org/abs/1706.03762?context=cs) mechanism. This means that these models can compute different embeddings for a word depending on the context.
There are tons of models based on this architecture, but the most famous one is certainly [BERT](https://arxiv.org/pdf/1810.04805.pdf) which has been used in several applications in [NLP](https://arxiv.org/pdf/2103.11943.pdf) [NLP](https://arxiv.org/pdf/2010.00854.pdf) and in many [flavours](https://aclanthology.org/2020.emnlp-demos.6.pdf). An interesting variation of BERT for our work is [SBERT](https://arxiv.org/pdf/1908.10084.pdf), which, thanks to siamese and triplet network structures, can better derive sentences similarities. Since most of the proposed Transformer architectures have a limit on the number of tokens they can handle, document embeddings can be computed by dividing the text in chunks, finding the average of all the word embeddings in every chunk, and then averaging the chunks embeddings.

### 2.2) Review of Recent Studies
Topic modelling has its roots in the 1980s ([paper](https://www.sciencedirect.com/science/article/pii/S0306437920300703?casa_token=nJDJ5Nlh9T4AAAAA:2LmMdid6ESJ6R7T6UL5MlAkBP_b3kagZy9PgUQUTlNvAZFgqKvzpvHf5Z7jFOKyFbCyKXg#b14)), but really took off in the late 1990s thanks to methods such as [LSI](https://asistdl.onlinelibrary.wiley.com/doi/epdf/10.1002/%28SICI%291097-4571%28199009%2941%3A6%3C391%3A%3AAID-ASI1%3E3.0.CO%3B2-9?saml_referrer=) and especially [LDA](https://www.jmlr.org/papers/volume3/blei03a/blei03a.pdf?ref=https://githubhelp.com). Many methods based on LDA were designed over the last two decades ([cells](https://www.sciencedirect.com/science/article/pii/S0002929718301150) [hierachical](https://ieeexplore.ieee.org/abstract/document/8607040) [bioinformatics](https://d-nb.info/1116751054/34)). Despite their success, conventional Bayesian probabilistic topic models started to show signs of fatigue in the era of big data and deep learning [here](https://www.bing.com/search?q=topic+modelling+meets+deep+learing&cvid=99e100f4d08d4b0ba5f77714870cd94a&aqs=edge..69i57.7129j0j4&FORM=ANAB01&PC=U531).  Instead, models based on the use of Deep Learning techniques are becoming more and more popular ([survey](https://arxiv.org/abs/2103.00498)).
DL methods have been applied to topic modeling for document representation [LSTM](https://www.sciencedirect.com/science/article/abs/pii/S0950705119301182), for computing semantic representations of topics ([ATM](https://www.sciencedirect.com/science/article/abs/pii/S0306457319300500)) and to deal with short texts ([Attention](https://www.sciencedirect.com/science/article/pii/S1877050919306283) [survey](https://www.mdpi.com/1424-8220/22/3/852)).

The scientific community did not focus just on designing different methods that are then applied on traditional data (text), but in the years there has been a great effort in the application of topic modeling to different fields and for many purposes([LDA in various fields](https://link.springer.com/article/10.1007/s11042-018-6894-4)). Some interesting fields in which topic modeling has been used are: Marketing and Business management([AI](https://www.sciencedirect.com/science/article/pii/S0148296320307165?casa_token=CTXuKVFl_D4AAAAA:pO1BI9qqL7GEI3taA8mMPBaUwoDBryzpdBwzNjyL9rmDDt5hmbImS3xrm0lX6Q05XY1dybJ_xA) [Brands](https://journals.sagepub.com/doi/full/10.1177/10949968221088275) [marketing](https://link.springer.com/article/10.1007/s11573-018-0915-7) [Business analytics](https://www.sciencedirect.com/science/article/pii/S0378720617309254?casa_token=EtcTboz2mvEAAAAA:BR5vVJd4bRrnfAyxNOTMc6PllQ-oBSjpZBc4Pwv0gyNro0CYQhHaIpEXEM1HLqNJqkjzCnKdKw) [TM in Management research](https://journals.aom.org/doi/abs/10.5465/annals.2017.0099)), analysis of scientific publications ([COVID-19](https://www.jmir.org/2020/11/e21559) [Collaborative filtering](https://dl.acm.org/doi/abs/10.1145/2020408.2020480?casa_token=F_2xHH541asAAAAA:r-0r-Zigz5iZa8KgomJCP5tJG3jOBYvdXMePCg_MYr0HL_sOmkkSf8WMrhl1aUjh-iMhDekN1SjD) [Korea](https://koreascience.kr/article/JAKO201312855326461.page)), biology and medicine ([bioinformatics](https://springerplus.springeropen.com/articles/10.1186/s40064-016-3252-8) [biological and medicL datasets](https://bmcbioinformatics.biomedcentral.com/articles/10.1186/1471-2105-15-S11-S11) [proteins](https://bmcbioinformatics.biomedcentral.com/articles/10.1186/1471-2105-7-58)), [Software traceability](https://ieeexplore.ieee.org/abstract/document/6062077?casa_token=1fXi-6Dlo5gAAAAA:6259L_R_-_FReFTKM0KJztI60eJmKCxD_eQ5k34sVB8IHc4tJzC6unvRmF5Ol6dx64W-). 

In the tourism field there are publications in which topic modeling is used to discover preferences in travel itineraries ([here](https://www.sciencedirect.com/science/article/pii/S0261517719301268?casa_token=UpNSl5Bfl_sAAAAA:qzQVBCYYQf92agcRLozvZswTDS3Fo8zMsjaQRoCLHIegXNwFchyeYJM3Y-XovZw33gUYotVw6w)), to study customers opinions ([sentiment](https://www.tandfonline.com/doi/full/10.1080/19368623.2017.1310075?casa_token=C1gkPjUtESsAAAAA%3ACkoQnodjVC-2XNpop-F-CTXjGJ4g5Yyh31lIHqWlXLARpfcZTa1SGOruy_eBkwO2IaTy31Nt91vP) [hotel](https://www.sciencedirect.com/science/article/pii/S0261517719300020?casa_token=sKr7C-F5geUAAAAA:6P5kjPe36i5uT1EbZ1VU_w5AXeNWkN5UoqLlKIYqMZNX_Y5iNnrp17BySkt-OWj8zbRvG6BxFA)  [reviews](https://link.springer.com/article/10.1007/s40558-015-0035-y) [tripadvisor]([Mining meaning from online ratings and reviews: Tourist satisfaction analysis using latent dirichlet allocation - ScienceDirect](https://www.sciencedirect.com/science/article/pii/S0261517716301698?casa_token=rNu7wIeK6ZYAAAAA:udcc5pDK_g8AADXGTJouOYMTTxpLvlXdMPZWG2Xl9LXMOo85EdgfK8CJeOj_t1be-Zawmg))), bikesharing ([bike](https://link.springer.com/article/10.1007/s11067-017-9366-x#Tab1)) and to make recommendations ([geo topic model](https://dl.acm.org/doi/abs/10.1145/2433396.2433444?casa_token=ct2kPmAiUSQAAAAA:yihk4f46hwxdYcLqLE6jImF5ZZL7gFSII83VwZrovT0zaW1x4tt_LUPgmMu67IZ02cc0XhSWFrhv) [Collaborative filtering](https://link.springer.com/chapter/10.1007/978-3-319-14442-9_45)).



[Evaluation](https://link.springer.com/chapter/10.1007/978-3-030-80599-9_4#chapter-info)

# Usage of different embedding methods
Points:
- doc2vec should be better for large datasets with very unique vocabulary;
- doc2vec is language agnostic, but multiple language will not be aligned;
- universal sentence encoder is suggested for smaller and multilingual datasets;
- Doc2vec performs better;
- Results are really similar, except for universal-sentence-encoder which is way worse;
- Universal-sentence-encoder often finds just 2-3 topics and is really unstable in several tests;
- Chunking improves coherence but decreases diversity;
- With tourpedia, chunking reduces performances (by a lot) and the number of topics is small apart from universal-sentence-encoder which has 63.4 topics in average (against 10-12), diversity is 0.4 point worse, and coherence is a bit better. distiluse finds 23.1 topics and has a good diversity score, just 0.04 worse than the best test and best coherence scores.

Even if Top2Vec was initially designed to use Doc2Vec architecture to generate the embeddings of the documents and words, its framework offers other options. We tried these other models because they can handle multilingual datasets, and they are also suggested for smaller data sets by Top2Vec's authors. The models we tried are the following: universal-sentence-encoder-multilingual and distiluse-base-multilingual-cased. Also, we did some tests where the documents embeddings were computed by chunking the texts, obtaining the embeddings from such chunks, and then averaging the embeddings into one final representation. This should help in case of long texts and when the embedding model has a token limit. 
All the tests were done on the easytour dataset, because it is the only one not in English. The results of the tests we made are shown in the table []. Those were obtained by averaging the results of 10 iterations of the same test.
Even if the results are generally quite similar, we can definitely say that Doc2Vec is the best performing model and universal-sentence-encoder is (by far) the worst one. Furthermore, universal-sentence-encoder often finds just 2-3 topics and the test are really unstable (several tests with same parameters can give results that are really different from each other). Chunking improves coherence but decreases diversity.

## References
1. [[Background for Topic Modeling]]

## Code
1. 

#### Tags
#paper #topicmodeling 