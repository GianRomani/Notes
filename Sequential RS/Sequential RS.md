# Sequential RS
Created: 2022-06-14 11:25
#note

Sequential RSs try to understand and model the sequential user behaviors, the interactions between users and items, the evolution of users' preferences and item popularity over time. This means that SRs take the prior sequential interactions as a context to predict which items would be interacted in the near future -> it is easier to diversify the recommendations.
More specifically, given a sequence of user-item interactions $S={i_1,i_2,...,i_{|S|}}$, where each interactions $i_j=<u,a,v>$ is the triple composed by the user *u*, the user's action *a* and the item *v*, the recommedation list is generated by $R=argmax f(S)$. In some cases some meta data can be used, actions of different kinds are considered and context (time, location etc) can vary a lot.
Sub-types of sequential RS are:
- Session-based recommendation -> no long-term user histories are available.
- Session-aware recommendation -> both short-term and long-term user histories are available.

The goal of a SRS's prediction can be:
- **experience-based** -> predict the next behavior type that the user will exert given an item (is he going to like it, buy it or just view it?). [[Multi-task learning]] is a good approach for this type o problems;
- **transaction-based** -> predict the next item the user will interact with (regardless of the type of the interaction). GRU4Rec and similar models, memory models, Hierarchical Periodic Memory Network, ARNN, DREAM, MCPRN, CNNs (NextItNet, HierTCN), Attention based (NARM, SDM, SASRec, [[BERT4Rec]]);
- **interaction-based** -> generalization of the two types listed above, i.e. the RS should be able to understand user preferences in a way that could help us predict both the next item the user will interact with and the way he/she is going to interact with it. RNNs are the most used models -> TA-RLBL.

## Challenges in sequential RSs
We can identify 5 subclasses of challenges.

### Long User-Item Interaction sequences
Higher chance to have more complex and comprehensive dependencies over the multiple interactions inside it. 
Challenges:
- **Learning higher-order sequential dependencies** -> complicated multi-level cascading dependencies across multiple user-item interactions. Two basic approaches that address this challenge are: higher order Markov Chain and RNNs. They both have some limitations though -> RNN's structure is too strict and Markov Chain's oarameters grow exponentially with the order.
- **Learning long-term sequential dependencies** -> dependencies between interactions that are far from each other in a sequence. LSTM and GRU have been applied for this kind of tasks. Some modela wrongly assume that close items are correlated, even when this is not true -> mixture models that combine multiple sub-models with different temporal ranges. This models are still not good enough -> more reasearch is needed ([[Transformer]]?).

### User-Item Interaction sequences with flexible order
In some cases the order of the interactions is not important or an interaction can depend on the union of several past interactions -> we need to capture the collective sequential dependencies. Not much attention was put on this problem, just some models based on CNNs.

### User-Item Interactions sequences with noise
Because of some uncertainties in the behaviours of the users, we could have noisy and irrelevant interactions. Few attempts to solve this problem -> attention models or memory networks.

### User-Item interaction sequences with Heterogeneous Relations
Relation could be of different types and deliver different kinds of information, for example we could have both short-term and long-term sequences. Mixture models are the most used.

### User-Item Interaction sequences with Hierarchical structures
We can consider two kinds of hierarchical structures:
- hierarchical structure between meta data and user-item interactions -> users' demographics can determine the users' preferences in some degree;
- hierarchical structure between sub-sequences and user-item interactions ->one user-item interaction can include mmultiple sub-sequences.

Hirarchical embedding models and hirarchical attention networks ahve been used.




## Categorization of SRS approaches
![[Categorizarion of SRS approaches.png]]

### Traditional Sequence Models
Main approaches:
- **Sequential pattern mining** -> methods that mine frequent patterns on sequence data and then utilize them to guide the subsequent recommendations. Simple and straightforward methods but they generates redundant patterns and generally focus on popular items, losing infrequent patterns;
- **Markov chain models** -> transitions over user-item interactions are modeled in a sequence. Basic MC approaches compute the transition probability based on explicit observations, latent MC embedding approaches embed MC into an Euclidean space and the compute the transition probabilities between interactions based on their Euclidean distance (use [this](https://www.ijcai.org/Proceedings/15/Papers/293.pdf) paper as reference). MC-based RSs can capture only short-term dependencies;
- [[kNN]] -> in item-based KNN, we consider just the last interaction and recommend items that are the most similar to it, in session-based KNN, instead, the entire existing session is compared to all the past sessions (using Jaccard similarity for example)

### Latent representation models
These models first learn a latent representation of each user and item and then predict the subsequent user-item interactions by using such representations. More implicit and complex dependencies are captured. The most representative models for this category are:
- **Factorization machines** ->user-item interactions are factorized into latent factors by [[Matrix factorization]] or tensor factorization. In this case, instead of the ratings (as happens in [[Collaborative filtering]]), the matrix is composed of interactions.
- **Embeddings** -> the latent representation for users and items is learnt by encoding all the user-item interactions in a sequence into a latent space. These latent representations can be directly used to calculate the interaction score or can be fed to networks. This approach seems to be the most promising.

### Reinforcement Learning
Goal: update recommendations according to the interactions between users and the recommender systems, i.e. a positive reward is assigned if the user express his interest on a recommended item. It is generally formulated as a Markov decision process with the goal of maximizing the cumulative rewards in a set of interactions.
Problem: lack of interpretability.
### Deep Neural Network models
The often reach SOTA results but they have also some weaknesses, for example they largely emphasize item representation over user representation and often consider all interactions as they belong to only one type.
The most used approaches are:
- [[RNN]] (and variants) -> naturally made to model sequential dependencies. They have also some shortcomings: they don't consider the assumption of existence of noisy data and, even if they are able to deal with longer sequences, they can not really capture collective dependencies;
- [[CNN]] -> this kind of models puts all the interactions into a matrix and then treats them as an image. They do not have strong order assumption over the interactions in a sequence, they rather learn patterns within areas of the "image", so they cannot effectively capture long-term dependencies. The computing cost for CNNs are smaller than the ones for RNNs;
- [[GNN]] -> each interaction is a node and then a sequence is a path in the graph. Embeddings of users or items are learned on the graph. Great potentioal to provide explainable recommendations;
- [[Neural attention]] models -> they emphasize the interactions that are really relevant;
- **Memory networks** -> by using an external matrix, these models can capture the dependencies between any historical user-item interaction and the next one. This matrix stores and update the historical interactions in a sequence more explicitly and dynamically, i.e. it reduces the interference caused by irrelevant interactions;
- **Mixture models** -> the combine different models;

## Other important aspects

- **Context-aware SRS** -> context couldheavily influence the user's choice;
- **Social-aware SRS** -> others' behaviors or opinions often affect the users' choices;
- **Interactive SRS** -> Multi-time step recommendations, for example in e-commerce RS we should consider users' behaviors as continuous rather than isolated events;
- **Cross-domain SRS** -> there could be sequential dependencies between items from different domains (for example, the purchase of a car insurance after the purcahse of a car).


## References
1. [Sequential RS survey](https://www.ijcai.org/proceedings/2019/0883.pdf)
2. [DL for Sequential RS survey](https://arxiv.org/pdf/1905.01997.pdf)
3. [Sequential RS,PapersWithCode](https://paperswithcode.com/task/sequential-recommendation)
4. [Keras Sequential RS](https://keras.io/examples/structured_data/movielens_recommendations_transformers/)
5. [Session rs, PapersWithCode](https://paperswithcode.com/task/session-based-recommendations)
6. [Sequence-aware survey](https://arxiv.org/pdf/1802.08452.pdf)

## Code
1. [session-rec](https://github.com/rn5l/session-rec)

#### Tags
#survey #sequential_RS 